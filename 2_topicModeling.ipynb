{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up our libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bryant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "nb_start_time = time.time()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #download the latest stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryant/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (13,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1040239, 26)\n"
     ]
    }
   ],
   "source": [
    "# import our data\n",
    "\n",
    "df = pd.read_csv('mergedProjectsAbstracts.csv',encoding='utf-8-sig')\n",
    "\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1. Project Terms Contain 'Opioid'\n",
    "\n",
    "The fedreporter data have a project_terms field. A subset of projects\n",
    "contain 'opioid' in that field. Let's make a flag for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:17\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "df['projectTermsFlag'] = pd.to_numeric(np.where(\n",
    "    df['PROJECT_TERMS'].str.contains(\"opioid\", case=False, na=False), 1, ''))\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% flagged: 0.8569184581620185\n"
     ]
    }
   ],
   "source": [
    "print('% flagged: ' + str(100*df['projectTermsFlag'].value_counts()[1]/len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. 'Wiki Approach'\n",
    "\n",
    "If we read through the [opioid wikipedia page](https://en.wikipedia.org/wiki/Opioid) we can extract words to highlight in project abstracts that might indicate it is a project related to opioids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms/n-grams gleaned from wikipedia\n",
    "opioid_terms = ['opioid','opiate','morphine','heroin',\n",
    "                'percocet','vicoprofen','dextromethorphan','loperamide',\n",
    "                'naloxegol','hydrocodone','oxycodone','fentanyl',\n",
    "                'naloxone','analgesics','carfentanil','benzodiazepines',\n",
    "                'narcotic','opium','cocaine','codeine',\n",
    "                'pain relief','cancer pain','anesthesia','chronic pain',\n",
    "                'nerve pain','fibromyalgia','overdose','addiction',\n",
    "                'withdrawal','dependence','recreational use','euphoria',\n",
    "                'tolerance','controlled substance','over-prescription',\n",
    "                'peripheral nervous system','psychoactive','agonist',\n",
    "                'antagonist','blood-brain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:04:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryant/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# clean up the abstracts for text matching\n",
    "st = time.time()\n",
    "\n",
    "# a regex pattern to help eliminate punctuation\n",
    "nonchars = re.compile( r'\\W+|\\d+' )\n",
    "\n",
    "# function to elim punctuation and set to lower case\n",
    "def clean(text):\n",
    "    return re.sub(nonchars, \" \", text).lower()\n",
    "\n",
    "# cut the end coding that interferes with cleaning function\n",
    "# df = df[0:-1]\n",
    "\n",
    "# drop nulls, as they don't help our analysis\n",
    "df_denull = df[df.ABSTRACT.notnull()]\n",
    "\n",
    "# may have to drop weird end coding that interferes with cleaning function\n",
    "# df_denull = df_denull[0:-1]\n",
    "\n",
    "df_denull['cleanText'] = df_denull['ABSTRACT'].apply(clean)\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryant/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "\n",
    "# Count the appearances of our defined terms in each abstract\n",
    "def countTerm(text):\n",
    "    return len(re.findall(term,text))\n",
    "\n",
    "for term in opioid_terms:\n",
    "    df_denull[term] = df_denull['cleanText'].apply(countTerm)\n",
    "    \n",
    "# sum of all term frequencies by abstract\n",
    "df_denull['sumTermCounts'] = df_denull[opioid_terms].sum(axis=1)\n",
    "\n",
    "# set term threshold\n",
    "wikiThreshold = 2;\n",
    "\n",
    "def wikiFlag(row):\n",
    "    if row['sumTermCounts'] > wikiThreshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "df_denull['wikiTermsFlag'] = df_denull.apply(wikiFlag, axis=1)\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_denull['wikiTermsFlag'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Text analysis (Topic modeling)\n",
    "\n",
    "LDA (Latent Dirichlet Allocation) is a model used for discovering abstract topics from a collection of documents. These 'latent' topics can be discovered based on observed data -- words in the documents, in this case.\n",
    "\n",
    "To surface these topics, we create a matrix where each document is a row and each column is a word in the corpus vocabulary. The corpus vocabulary is the universe of words present in any one or more documents in the corpus (minus chosen 'stopwords' that we consider to provide little information).\n",
    "\n",
    "Each cell of the matrix would be a count of that word in that document. A variant increases the level of sophistication by using a normalized version of these counts known as TF-IDF. TF stands for term-frequency and TF-IDF is term-frequency times inverse document-frequency. In other words, we are not only looking for how often a word appears in a given document, but also whether this particular word is distinct across all the collections of documents (corpus). For example, intuitively we understand that words like \"often\" or \"use\" are more frequently encountered, but they are less informative (more semantically-vacuous) if we want to discern a particular topic of a document, as they might be frequently encounter across all text documents in a corpus. On the other hand, words which we will see less frequently across a collection of document might indicate that those words are specific to a particular document, and, therefore, constitute a basis for a topic.\n",
    "\n",
    "We provide the model with this matrix and how many topics we want it to use. Think of it like a k-means clustering analog. The model will then iterate a specified number of times considering two distributions; 1) which words in the vocabulary are more or less probable to belong in a given topic and 2) which topic is more or less probable for a given document.\n",
    "\n",
    "The main assumptions, if this all went over your head:\n",
    "* each document consists of a mixture of topics, and\n",
    "* each topic consists of a collection of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our corpus (limit number of rows while perfecting code)\n",
    "df_modeling = df_denull[:5000]\n",
    "# df_modeling = df_denull # to run on whole thing...\n",
    "\n",
    "# Get only the text of abstracts\n",
    "corpus = df_modeling['ABSTRACT'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words and stemming them\n",
    "stemmer = nltk.SnowballStemmer('english')\n",
    "eng_stopwords = stopwords.words('english')\n",
    "add_stopwords = ['understand','method']\n",
    "comb_stopwords = eng_stopwords + add_stopwords\n",
    "stemmed_stopwords = []\n",
    "\n",
    "for w in comb_stopwords:\n",
    "    stemmed_stopwords.append(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "'''\n",
    "Before we can apply LDA, we need to create a vocabulary with all the words in our data\n",
    "We specify to only include those words that appear in less than 10% of the document \n",
    "and appear in at least 0.5% of documents. \n",
    "docs: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "The exercise of running the vectorizer on each year independently, then only\n",
    "keeping ngrams that all 10 years of abstracts possess is all geared towards breaking up\n",
    "the computation into manageable chunks that won't crash the kernel.\n",
    "'''\n",
    "\n",
    "# prepare our vectorizer\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "# our stemming function\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                  max_df=0.10,\n",
    "                                  min_df=0.005,\n",
    "#                                   max_features = 1500, # useful for limiting coderun duration\n",
    "                                  ngram_range = (0,2),\n",
    "                                  stop_words=stemmed_stopwords)\n",
    "\n",
    "doc_term_matrix = stem_vectorizer.fit_transform(corpus)\n",
    "doc_term_features = stem_vectorizer.get_feature_names()\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using below code, we can print out the many, many words excluded due to:\n",
    "- occurred in too many documents (max_df)\n",
    "- occurred in too few documents (min_df)\n",
    "- were cut off by feature selection (max_features)\n",
    "'''\n",
    "\n",
    "# print(count_vect.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Use LDA to create topics.\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=200, random_state=1)  \n",
    "LDA.fit(doc_term_matrix)\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list for topics\n",
    "topicList = []\n",
    "\n",
    "# fill out topics list as top 20 words in each topic\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    ithTopic = [doc_term_features[i] for i in topic.argsort()[-20:]]\n",
    "    topicList.append(ithTopic)\n",
    "    \n",
    "topicListDf = pd.DataFrame(topicList)\n",
    "# topicListDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "'''\n",
    "Assign the probability of all the topics to each document, then\n",
    "add a column to the original data frame that will store the highest-scoring\n",
    "topic for that abstract.\n",
    "'''\n",
    "\n",
    "# matrix where each row is an abstract, each column a topic. Each cell is value of that topic for that abstract.\n",
    "topic_values = LDA.transform(doc_term_matrix)  \n",
    "\n",
    "'''\n",
    "take the column number associated with the highest value in a given row, store in our analytical dataframe\n",
    "'''\n",
    "df_modeling['primeTopicId'] = topic_values.argmax(axis=1)\n",
    "\n",
    "'''\n",
    "store the valence of that prime topic for that abstract as well\n",
    "'''\n",
    "df_modeling['primeTopicValence'] = topic_values.max(axis=1)\n",
    "\n",
    "print(\"Shape of topic_values: \",(topic_values.shape))\n",
    "\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time() - start_time)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which topics are most common among projects tagged explicitly?\n",
    "pd.DataFrame(df_modeling[df_modeling.wikiTermsFlag == 1].primeTopicId.value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the top topic consist of?\n",
    "topicList[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time to run whole notebook: ')\n",
    "elapsed_time = time.time() - nb_start_time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
