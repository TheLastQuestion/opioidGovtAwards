{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bryant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from Julia Lane course and https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bryant/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (6,13,19,20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "file = 'opioidRQ3_constructedDataset.csv'\n",
    "df = (pd.read_csv(file,skipinitialspace=True,encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df09 = df[df.FY == 2009]\n",
    "df10 = df[df.FY == 2010]\n",
    "df11 = df[df.FY == 2011]\n",
    "df12 = df[df.FY == 2012]\n",
    "df13 = df[df.FY == 2013]\n",
    "df14 = df[df.FY == 2014]\n",
    "df15 = df[df.FY == 2015]\n",
    "df16 = df[df.FY == 2016]\n",
    "df17 = df[df.FY == 2017]\n",
    "df18 = df[df.FY == 2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "# we can add our own stopwords here, but max_df should handle it for us...\n",
    "domain_stopwords = ['experiments','exploration','exploratory','explore','experiment','findings','financial',\n",
    "                   'experimental','finally','far','five','find','extent']\n",
    "\n",
    "# modified_stopwords = eng_stopwords + domain_stopwords\n",
    "modified_stopwords = eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# This section lays out what is happening on small chunks of data and is for explanation purposes only\n",
    "# '''\n",
    "# # subset to a few rows on three years' of data for fast run\n",
    "# df09_small = df09[0:500]\n",
    "# df10_small = df10[0:500]\n",
    "# df11_small = df11[0:500]\n",
    "\n",
    "# '''\n",
    "# define a function that breaks each abstract into words/n-grams. \n",
    "# Each row is an abstract, each column a ngram, and each cell \n",
    "# indicates whether the ngram of that column is present in that row's abstract\n",
    "# '''\n",
    "# count_vect = CountVectorizer(\n",
    "#     max_df=0.05, \n",
    "#     min_df=10, \n",
    "#     ngram_range = (0,2),\n",
    "# #     max_features = 500,\n",
    "#     stop_words=modified_stopwords)\n",
    "\n",
    "# # run the function 2009 and turn resulting sparse matrix into a dataframe\n",
    "# doc_term_matrix_09 = count_vect.fit_transform(df09_small['ABSTRACT'].values.astype('U'))\n",
    "# doc_term_df_09 = pd.DataFrame(doc_term_matrix_09.toarray(),columns=count_vect.get_feature_names(),index=df09_small.PROJECT_ID)\n",
    "\n",
    "# doc_term_matrix_10 = count_vect.fit_transform(df10_small['ABSTRACT'].values.astype('U'))\n",
    "# doc_term_df_10 = pd.DataFrame(doc_term_matrix_10.toarray(),columns=count_vect.get_feature_names(),index=df10_small.PROJECT_ID)\n",
    "\n",
    "# doc_term_matrix_11 = count_vect.fit_transform(df11_small['ABSTRACT'].values.astype('U'))\n",
    "# doc_term_df_11 = pd.DataFrame(doc_term_matrix_11.toarray(),columns=count_vect.get_feature_names(),index=df11_small.PROJECT_ID)\n",
    "\n",
    "# # # append the resulting dataframes together\n",
    "# result_raw = doc_term_df_09.append([doc_term_df_10, doc_term_df_11],sort=False)\n",
    "\n",
    "# # # keep only n-grams that appear in every year of data\n",
    "# result_ready = result_raw.dropna(axis=1)\n",
    "\n",
    "# # # run LDA\n",
    "# LDA = LatentDirichletAllocation(n_components=100, random_state=1)  \n",
    "# LDA.fit(result_ready) \n",
    "\n",
    "# '''\n",
    "# by design, the each topic consists of all words in the vocabulary, along with\n",
    "# probability values. Here we print the 15 words with highest prov value within each topic.\n",
    "# '''\n",
    "# # initialize list for topics\n",
    "# topicList = []\n",
    "\n",
    "# for i,topic in enumerate(LDA.components_):  \n",
    "#     ithTopic = [result_ready.columns[i] for i in topic.argsort()[-15:]]\n",
    "#     topicList.append(ithTopic)\n",
    "    \n",
    "# pd.DataFrame(topicList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219965, 1450)\n",
      "(312931, 1376)\n",
      "(399337, 1348)\n",
      "(486146, 1324)\n",
      "(572738, 1297)\n",
      "(661267, 1272)\n",
      "(750764, 1256)\n",
      "(836570, 1236)\n",
      "(928683, 1205)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Actual run.\n",
    "\n",
    "Before we can apply LDA, we need to create vocabulary of all the words in our data\n",
    "We specify to only include those words that appear in less than 10% of the document \n",
    "and appear in at least 5% of documents. \n",
    "docs: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "The exercise of running the vectorizer on each year independently, then only\n",
    "keeping ngrams that all 10 years of abstracts possess is all geared towards breaking up\n",
    "the computation into manageable chunks that won't crash the kernel.\n",
    "'''\n",
    "# define our vectorizer\n",
    "count_vect = CountVectorizer(\n",
    "    max_df=0.10,\n",
    "    min_df=0.005, \n",
    "    max_features = 1500,\n",
    "    ngram_range = (0,2),\n",
    "    stop_words=modified_stopwords)\n",
    "\n",
    "# create a list of the latter 9 dataframes\n",
    "chunks = [df10,df11,df12,df13,df14,df15,df16,df17,df18]\n",
    "\n",
    "'''\n",
    "in our eventual loop, we will be appending to a base dataframe, \n",
    "so we initialize it with 2009 data and append from there\n",
    "'''\n",
    "chunk = df09\n",
    "\n",
    "# # base dataframe\n",
    "# vectorize\n",
    "doc_term_matrix = count_vect.fit_transform(chunk['ABSTRACT'].values.astype('U'))\n",
    "# convert to pd\n",
    "base_doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=count_vect.get_feature_names(),index=chunk.PROJECT_ID)\n",
    "\n",
    "# vectorize and append rest of the years\n",
    "for chunk in chunks:\n",
    "    # vectorize\n",
    "    doc_term_matrix = count_vect.fit_transform(chunk['ABSTRACT'].values.astype('U'))\n",
    "    # convert to pd\n",
    "    doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=count_vect.get_feature_names(),index=chunk.PROJECT_ID)\n",
    "    # append\n",
    "    result_raw = base_doc_term_df.append(doc_term_df,sort=False)\n",
    "    # only keep only words/n-grams that appear in every year of data\n",
    "    result_ready = result_raw.dropna(axis=1)\n",
    "    # save result as base in preparation for next loop\n",
    "    base_doc_term_df = result_ready\n",
    "    print(base_doc_term_df.shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='batch', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=800, n_jobs=None, n_topics=None, perp_tol=0.1,\n",
       "             random_state=1, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Use LDA to create topics.\n",
    "'''\n",
    "LDA = LatentDirichletAllocation(n_components=800, random_state=1)  \n",
    "LDA.fit(base_doc_term_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize list for topics\n",
    "topicList = []\n",
    "\n",
    "for i,topic in enumerate(LDA.components_):  \n",
    "    ithTopic = [result_ready.columns[i] for i in topic.argsort()[-15:]]\n",
    "    topicList.append(ithTopic)\n",
    "    \n",
    "topics_full_run = pd.DataFrame(topicList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Assign the probability of all the topics to each document, then\n",
    "add a column to the original data frame that will store the highest-scoring\n",
    "topic for that abstract.\n",
    "'''\n",
    "topic_values = LDA.transform(base_doc_term_df)  \n",
    "df['primeTopicId'] = topic_values.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primeTopicId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     primeTopicId\n",
       "547          1269\n",
       "575           282\n",
       "398           278\n",
       "562           225\n",
       "308           219\n",
       "741           216\n",
       "221           185\n",
       "153           182\n",
       "125           153\n",
       "303           113"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# which topics are most common among projects tagged explicitly?\n",
    "pd.DataFrame(df[df.opioid_num == 1].primeTopicId.value_counts())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modulation',\n",
       " 'report',\n",
       " 'poorly',\n",
       " 'without',\n",
       " 'condition',\n",
       " 'treating',\n",
       " 'therapies',\n",
       " 'clinically',\n",
       " 'problem',\n",
       " 'reduce',\n",
       " 'often',\n",
       " 'treat',\n",
       " 'treatments',\n",
       " 'chronic',\n",
       " 'pain']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which topic is affiliated with the most projects tagged with opioid project term?\n",
    "topicList[547]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's skim all the projects in both that topic and tagged with opioid in the project terms\n",
    "# df[(df.opioid_num == 1) & (df.primeTopicId == 547)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DESCRIPTION (provided by applicant): Pain-related disorders cause an incalculable toll in human suffering and present a significant economic problem. The development of new treatments for these disorders is hindered by a lack of information about the basic neural mechanisms that process pain-related information. To date, investigations of the dynamic activation of mechanisms that facilitate pain have provided substantial insights into the neurophysiology and neuropharmacology of chronic pain. However, understanding of the dynamic response properties of inhibitory mechanisms has remained limited, despite the fact that disruption of inhibition may also contribute substantially to chronic pain. A recently identified analgesic phenomenon, offset analgesia, provides a powerful tool for the study of dynamic activation of inhibitory mechanisms. A series of psychophysical studies in humans subjects will systematically delineate the neurophysiological and neuropharmacological mechanisms that initiate and maintain offset analgesia during acute pain states, and determine the contribution of disrupted offset analgesia to pathophysiological pain states. Together, these studies will significantly enhance our knowledge of basic neural mechanisms underlying pain and will provide a foundation for the development of novel treatments for pain.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ABSTRACT[17606]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>PROJECT_TERMS</th>\n",
       "      <th>PROJECT_TITLE</th>\n",
       "      <th>DEPARTMENT</th>\n",
       "      <th>AGENCY</th>\n",
       "      <th>IC_CENTER</th>\n",
       "      <th>PROJECT_NUMBER</th>\n",
       "      <th>PROJECT_START_DATE</th>\n",
       "      <th>PROJECT_END_DATE</th>\n",
       "      <th>...</th>\n",
       "      <th>naloxone</th>\n",
       "      <th>narcotic</th>\n",
       "      <th>opium</th>\n",
       "      <th>cocaine</th>\n",
       "      <th>codeine</th>\n",
       "      <th>pain</th>\n",
       "      <th>analgesics</th>\n",
       "      <th>sumTermCounts</th>\n",
       "      <th>tagCompare</th>\n",
       "      <th>primeTopicId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>103915</td>\n",
       "      <td>base; Cities; Learning; Mission; next generati...</td>\n",
       "      <td>EDUCATION IN ACTION NASA EXCHANGE CITY LEARNIN...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNX09AR64G</td>\n",
       "      <td>10/1/2009</td>\n",
       "      <td>9/30/2010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>103916</td>\n",
       "      <td>Development; Future; programs; Science; Techno...</td>\n",
       "      <td>EDUCATIONAL ADVANCEMENT ALLIANCE INC MATH SCIE...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNX09AQ21G</td>\n",
       "      <td>8/1/2009</td>\n",
       "      <td>7/31/2010</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>103917</td>\n",
       "      <td>Development; Educational process of instructin...</td>\n",
       "      <td>CUBRC, INC FY09 EARMARK ENTITLED, ''TO CONTINU...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNX09AT31G</td>\n",
       "      <td>10/1/2010</td>\n",
       "      <td>9/30/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>103918</td>\n",
       "      <td>Joints; Life; programs; Request for Proposals;...</td>\n",
       "      <td>UNIVERSITY CORPORATION FOR ATMOSPHERIC RESEARC...</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NASA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NNX09AW48A</td>\n",
       "      <td>10/1/2009</td>\n",
       "      <td>9/30/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>103919</td>\n",
       "      <td>Area; base; Computer Architectures; design; De...</td>\n",
       "      <td>PLANNING FUTURE RESEARCH IN NETWORK SCIENCE AN...</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NSF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0962520</td>\n",
       "      <td>6/1/2009</td>\n",
       "      <td>2/28/2011</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neither</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  PROJECT_ID                                      PROJECT_TERMS  \\\n",
       "0           0      103915  base; Cities; Learning; Mission; next generati...   \n",
       "1           1      103916  Development; Future; programs; Science; Techno...   \n",
       "2           2      103917  Development; Educational process of instructin...   \n",
       "3           3      103918  Joints; Life; programs; Request for Proposals;...   \n",
       "4           4      103919  Area; base; Computer Architectures; design; De...   \n",
       "\n",
       "                                       PROJECT_TITLE DEPARTMENT AGENCY  \\\n",
       "0  EDUCATION IN ACTION NASA EXCHANGE CITY LEARNIN...       NASA   NASA   \n",
       "1  EDUCATIONAL ADVANCEMENT ALLIANCE INC MATH SCIE...       NASA   NASA   \n",
       "2  CUBRC, INC FY09 EARMARK ENTITLED, ''TO CONTINU...       NASA   NASA   \n",
       "3  UNIVERSITY CORPORATION FOR ATMOSPHERIC RESEARC...       NASA   NASA   \n",
       "4  PLANNING FUTURE RESEARCH IN NETWORK SCIENCE AN...        NSF    NSF   \n",
       "\n",
       "  IC_CENTER PROJECT_NUMBER PROJECT_START_DATE PROJECT_END_DATE     ...       \\\n",
       "0       NaN     NNX09AR64G          10/1/2009        9/30/2010     ...        \n",
       "1       NaN     NNX09AQ21G           8/1/2009        7/31/2010     ...        \n",
       "2       NaN     NNX09AT31G          10/1/2010        9/30/2011     ...        \n",
       "3       NaN     NNX09AW48A          10/1/2009        9/30/2011     ...        \n",
       "4       NaN        0962520           6/1/2009        2/28/2011     ...        \n",
       "\n",
       "  naloxone narcotic  opium cocaine codeine pain analgesics sumTermCounts  \\\n",
       "0        0        0      0       0       0    0          0             0   \n",
       "1        0        0      0       0       0    0          0             0   \n",
       "2        0        0      0       0       0    0          0             0   \n",
       "3        0        0      0       0       0    0          0             0   \n",
       "4        0        0      0       0       0    0          0             0   \n",
       "\n",
       "  tagCompare primeTopicId  \n",
       "0    neither           35  \n",
       "1    neither          565  \n",
       "2    neither           32  \n",
       "3    neither          113  \n",
       "4    neither           46  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see what we have before saving to a csv\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('opioidRQ5_constructedDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(topicList).to_csv('opioidRQ5_topicList.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = stopwords.words('english')\n",
    "stemmed_stopwords = []\n",
    "\n",
    "for w in eng_stopwords:\n",
    "    stemmed_stopwords.append(stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
