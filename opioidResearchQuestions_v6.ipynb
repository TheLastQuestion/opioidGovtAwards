{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bryant/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adapted from Julia Lane course and https://stackabuse.com/python-for-nlp-topic-modeling/\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #download the latest stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "# 10 years of project data\n",
    "fiscal_years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
    "prefix = 'FedRePORTER_PRJ_C_FY'\n",
    "suffix = '.csv'\n",
    "\n",
    "# initialize dataframe with fy09 data\n",
    "file = 'FedRePORTER_PRJ_C_FY2009.csv'\n",
    "print('Reading in ' + file)\n",
    "projects_df = (pd.read_csv(file,skipinitialspace=True,encoding='utf-8'))\n",
    "\n",
    "# concatenate 10 years of data\n",
    "for year in fiscal_years:\n",
    "    file = prefix + year + suffix\n",
    "    print('Reading in ' + file)\n",
    "    projects_df = projects_df.append(pd.read_csv(file, skipinitialspace=True, encoding='utf-8'), ignore_index=True)\n",
    "\n",
    "# new variable is 1 for rows with opioid in project term column\n",
    "projects_df['opioid'] = np.where(\n",
    "    projects_df['PROJECT_TERMS'].str.contains(\"opioid\",case=False, na=False), 1, '')\n",
    "\n",
    "# create a numeric version of our flag\n",
    "projects_df['opioid_num'] = pd.to_numeric(projects_df['opioid'])\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# 10 years of abstracts data\n",
    "fiscal_years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
    "prefix = 'FedRePORTER_PRJABS_C_FY'\n",
    "suffix = '.csv'\n",
    "\n",
    "# initialize dataframe with fy09 data\n",
    "file = 'FedRePORTER_PRJABS_C_FY2009.csv'\n",
    "print('Reading in ' + file)\n",
    "abstracts_df = (pd.read_csv(file,skipinitialspace=True,encoding='utf-8'))\n",
    "\n",
    "for year in fiscal_years:\n",
    "    file = prefix + year + suffix\n",
    "    print('Reading in ' + file)\n",
    "    abstracts_df = abstracts_df.append(pd.read_csv(file, skipinitialspace=True, encoding='utf-8'), ignore_index=True)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affilated terms from https://en.wikipedia.org/wiki/Opioid\n",
    "opioid_terms = ['opioid','morphine','pain relief','anesthesia',\n",
    "                'overdose','addiction','withdrawal',\n",
    "                'controlled substance','over-prescription','heroin',\n",
    "               'opiate','hydrocodone','oxycodone','fentanyl','naloxone',\n",
    "               'narcotic','opium','cocaine','codeine','pain',\n",
    "                'analgesics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# keep only variables needed and remove missing values\n",
    "abstracts_10yrs = abstracts_df.dropna()\n",
    "\n",
    "# cut the end coding that interferes with cleaning script\n",
    "abstracts_10yrs = abstracts_10yrs[0:-1]\n",
    "\n",
    "#get rid of the punctuations and set all characters to lowercase\n",
    "nonchars = re.compile( r'\\W+|\\d+' )\n",
    "\n",
    "def clean(text):\n",
    "    return re.sub(nonchars, \" \", text).lower()\n",
    "\n",
    "abstracts_10yrs['cleanText'] = abstracts_10yrs['ABSTRACT'].apply(clean)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Count the appearances of our defined terms in each abstract\n",
    "def countTerm(text):\n",
    "    return len(re.findall(term,text))\n",
    "\n",
    "for term in opioid_terms:\n",
    "    abstracts_10yrs[term] = abstracts_10yrs['cleanText'].apply(countTerm)\n",
    "    \n",
    "# sum of all term frequencies by abstract\n",
    "abstracts_10yrs['sumTermCounts'] = abstracts_10yrs[opioid_terms].sum(axis=1)\n",
    "\n",
    "print('The number of abstracts with three or more opioid terms is: ' +\n",
    "     str(abstracts_10yrs[abstracts_10yrs['sumTermCounts']>2].shape[0]))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# merge projects and abstracts by PROJECT_ID\n",
    "merged_df_10yrs = pd.merge(projects_df, abstracts_10yrs, on='PROJECT_ID')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define compareTags function\n",
    "\n",
    "wikiThreshold = 2;\n",
    "\n",
    "def compareTags(row):\n",
    "    if (row['opioid_num'] == 1) & (row['sumTermCounts'] > wikiThreshold):\n",
    "        return 'both'\n",
    "    if row['opioid_num'] == 1:\n",
    "        return 'explicitOnly'\n",
    "    if row['sumTermCounts'] > wikiThreshold:\n",
    "        return 'wikiOnly'\n",
    "    return 'neither'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# compare 10yr data tags\n",
    "merged_df_10yrs['tagCompare'] = merged_df_10yrs.apply(compareTags, axis=1)\n",
    "\n",
    "print('Number of projects')\n",
    "merged_df_10yrs['tagCompare'].value_counts()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cost sums')\n",
    "print('Neither:       ' + str(merged_df_10yrs.FY_TOTAL_COST[merged_df_10yrs.tagCompare == 'neither'].sum()))\n",
    "print('Both:          ' + str(merged_df_10yrs.FY_TOTAL_COST[merged_df_10yrs.tagCompare == 'both'].sum()))\n",
    "print('Explicit only: ' + str(merged_df_10yrs.FY_TOTAL_COST[merged_df_10yrs.tagCompare == 'explicitOnly'].sum()))\n",
    "print('Wiki only:     ' + str(merged_df_10yrs.FY_TOTAL_COST[merged_df_10yrs.tagCompare == 'wikiOnly'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv with progress so far\n",
    "start_time = time.time()\n",
    "merged_df_10yrs.to_csv('part1_OpioidAnalyticalData.csv')\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:05:26'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import csv, if previous cells weren't run this session\n",
    "start_time = time.time()\n",
    "\n",
    "file = 'opioidRQ3_constructedDataset.csv'\n",
    "df = (pd.read_csv(file,skipinitialspace=True,encoding='utf-8',\n",
    "                 dtype={'PROJECT_ID': object,\n",
    "                        'PROJECT_TERMS': object,\n",
    "                        'PROJECT_TITLE': object,\n",
    "                        'DEPARTMENT': str,\n",
    "                        'AGENCY': str,\n",
    "                        'PROJECT_START_DATE': str,\n",
    "                        'PROJECT_END_DATE': str,\n",
    "                        'ORGANIZATION_CITY': str,\n",
    "                        'CFDA_CODE': str,\n",
    "                        'FY': int,\n",
    "                        'FY_TOTAL_COST': float,\n",
    "                        'FY_TOTAL_COST_SUB_PROJECTS': float                     \n",
    "                       }))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00:00:41'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-chunk the data for topic modeling\n",
    "start_time = time.time()\n",
    "\n",
    "df09 = df[df.FY == 2009]\n",
    "df10 = df[df.FY == 2010]\n",
    "df11 = df[df.FY == 2011]\n",
    "df12 = df[df.FY == 2012]\n",
    "df13 = df[df.FY == 2013]\n",
    "df14 = df[df.FY == 2014]\n",
    "df15 = df[df.FY == 2015]\n",
    "df16 = df[df.FY == 2016]\n",
    "df17 = df[df.FY == 2017]\n",
    "df18 = df[df.FY == 2018]\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare stopwords\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "# we can add our own stopwords here, but max_df should handle it for us...\n",
    "domain_stopwords = ['experiments','exploration','exploratory','explore','experiment','findings','financial',\n",
    "                   'experimental','finally','far','five','find','extent']\n",
    "\n",
    "# modified_stopwords = eng_stopwords + domain_stopwords\n",
    "modified_stopwords = eng_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Actual run.\n",
    "\n",
    "Before we can apply LDA, we need to create vocabulary of all the words in our data\n",
    "We specify to only include those words that appear in less than 10% (max_df) of the document \n",
    "and appear in at least 5% (min_df) of documents. For computation reasons, limit to 'max_features.'\n",
    "docs: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "The exercise of running the vectorizer on each year independently, then only\n",
    "keeping ngrams that all 10 years of abstracts possess is all geared towards breaking up\n",
    "the computation into manageable chunks that won't crash the kernel.\n",
    "'''\n",
    "# Create a Snowball stemmer \n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# define our vectorizer\n",
    "count_vect = CountVectorizer(\n",
    "    max_df=0.10,\n",
    "    min_df=0.005, \n",
    "    max_features = 1500,\n",
    "#     ngram_range = (0,2),\n",
    "    stop_words=modified_stopwords)\n",
    "\n",
    "# create a list of the latter 9 dataframes\n",
    "chunks = [df10,df11,df12,df13,df14,df15,df16,df17,df18]\n",
    "\n",
    "'''\n",
    "in our eventual loop, we will be appending to a base dataframe, \n",
    "so we initialize it with 2009 data and append from there\n",
    "'''\n",
    "chunk = df09\n",
    "\n",
    "# # base dataframe\n",
    "# vectorize\n",
    "doc_term_matrix = count_vect.fit_transform(chunk['ABSTRACT'].values.astype('U'))\n",
    "# convert to pd\n",
    "base_doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=count_vect.get_feature_names(),index=chunk.PROJECT_ID)\n",
    "\n",
    "# vectorize and append rest of the years\n",
    "for chunk in chunks:\n",
    "    # vectorize\n",
    "    doc_term_matrix = count_vect.fit_transform(chunk['ABSTRACT'].values.astype('U'))\n",
    "    # convert to pd\n",
    "    doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=count_vect.get_feature_names(),index=chunk.PROJECT_ID)\n",
    "    # append\n",
    "    result_raw = base_doc_term_df.append(doc_term_df,sort=False)\n",
    "    # only keep only words/n-grams that appear in every year of data\n",
    "    result_ready = result_raw.dropna(axis=1)\n",
    "    # save result as base in preparation for next loop\n",
    "    base_doc_term_df = result_ready\n",
    "    print(base_doc_term_df.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 800)\t1\n",
      "  (0, 187)\t1\n",
      "  (0, 1292)\t1\n",
      "  (0, 910)\t1\n",
      "  (0, 715)\t1\n",
      "  (0, 864)\t1\n",
      "  (0, 756)\t1\n",
      "  (0, 229)\t1\n",
      "  (0, 507)\t1\n",
      "  (0, 897)\t3\n",
      "  (0, 39)\t1\n",
      "  (1, 583)\t1\n",
      "  (1, 522)\t1\n",
      "  (1, 1491)\t1\n",
      "  (1, 578)\t1\n",
      "  (1, 825)\t1\n",
      "  (1, 698)\t1\n",
      "  (2, 1341)\t1\n",
      "  (2, 1008)\t1\n",
      "  (2, 571)\t2\n",
      "  (2, 481)\t1\n",
      "  (2, 436)\t1\n",
      "  (2, 608)\t1\n",
      "  (2, 698)\t1\n",
      "  (2, 864)\t1\n",
      "  :\t:\n",
      "  (499, 538)\t1\n",
      "  (499, 1077)\t1\n",
      "  (499, 1025)\t1\n",
      "  (499, 966)\t1\n",
      "  (499, 707)\t1\n",
      "  (499, 994)\t1\n",
      "  (499, 805)\t1\n",
      "  (499, 976)\t2\n",
      "  (499, 292)\t1\n",
      "  (499, 454)\t1\n",
      "  (499, 1016)\t1\n",
      "  (499, 530)\t2\n",
      "  (499, 1357)\t2\n",
      "  (499, 1041)\t2\n",
      "  (499, 119)\t1\n",
      "  (499, 383)\t1\n",
      "  (499, 911)\t1\n",
      "  (499, 261)\t1\n",
      "  (499, 1236)\t1\n",
      "  (499, 429)\t1\n",
      "  (499, 1111)\t5\n",
      "  (499, 120)\t1\n",
      "  (499, 822)\t1\n",
      "  (499, 1414)\t1\n",
      "  (499, 910)\t1\n",
      "['000', '08', '09', '10', '100', '1000', '12', '13', '14', '15', '20', '2010', '21st', '25', '30', '3d', '40', '589', '64257', 'abil', 'abl', 'abov', 'abstract', 'abund', 'academ', 'academi', 'acceler', 'accept', 'access', 'accomplish', 'account', 'accumul', 'accur', 'accuraci', 'achiev', 'acidif', 'acm', 'acquir', 'acquisit', 'action', 'actual', 'actuat', 'ad', 'adapt', 'add', 'adequ', 'adjust', 'administr', 'adopt', 'adult', 'advantag', 'affect', 'affin', 'africa', 'african', 'after', 'against', 'age', 'agenc', 'agenda', 'agent', 'ago', 'agricultur', 'aid', 'aim', 'air', 'alaska', 'algebra', 'algorithm', 'alloc', 'alloy', 'almost', 'along', 'alreadi', 'alter', 'altern', 'although', 'america', 'among', 'amount', 'anachronist', 'analog', 'analys', 'analyt', 'analyz', 'ancient', 'angl', 'ani', 'anim', 'annual', 'anoth', 'anoxia', 'answer', 'anticip', 'antineutrino', 'appear', 'appropri', 'approxim', 'aquif', 'archaeolog', 'architectur', 'archiv', 'arid', 'aris', 'arizona', 'around', 'array', 'art', 'articl', 'artifact', 'asia', 'ask', 'aspect', 'assembl', 'assign', 'assist', 'assum', 'assumpt', 'astronom', 'astronomi', 'astrophys', 'asymptot', 'atlant', 'atmospher', 'atom', 'attach', 'attack', 'attempt', 'attend', 'attent', 'attract', 'attribut', 'audienc', 'autom', 'automat', 'autonom', 'avoid', 'awar', 'back', 'background', 'bacteri', 'bacteria', 'balanc', 'band', 'bank', 'barnacl', 'barrier', 'basi', 'basic', 'basin', 'beach', 'beam', 'becom', 'befor', 'begin', 'believ', 'best', 'beta', 'better', 'beyond', 'bias', 'billion', 'bind', 'bio', 'biodivers', 'biofilm', 'biogeochem', 'bioherm', 'biospher', 'biotic', 'black', 'block', 'board', 'bodi', 'bond', 'book', 'bound', 'boundari', 'brain', 'bridg', 'broaden', 'budget', 'built', 'busi', 'calcul', 'calibr', 'california', 'call', 'camp', 'campaign', 'campus', 'canada', 'cancer', 'cannot', 'capabl', 'capac', 'captur', 'carbon', 'care', 'career', 'carolina', 'carri', 'case', 'categori', 'caus', 'causal', 'cave', 'cell', 'cellular', 'census', 'central', 'centuri', 'certain', 'chain', 'channel', 'character', 'characterist', 'charg', 'chemic', 'chemistri', 'children', 'china', 'chines', 'chip', 'choic', 'chosen', 'chronolog', 'circuit', 'circul', 'citi', 'citizen', 'civil', 'class', 'classic', 'classroom', 'clean', 'clear', 'climat', 'close', 'closur', 'cloud', 'cluster', 'co', 'co2', 'coast', 'coastal', 'coat', 'code', 'cognit', 'coher', 'cold', 'colleagu', 'colleg', 'colon', 'color', 'come', 'commerci', 'commit', 'committe', 'common', 'communic', 'compar', 'comparison', 'compel', 'compet', 'competit', 'compil', 'complement', 'complementari', 'complet', 'composit', 'compound', 'comprehens', 'compris', 'concentr', 'concept', 'concern', 'conclus', 'concurr', 'condit', 'confer', 'confid', 'configur', 'confin', 'conflict', 'conjectur', 'conjunct', 'connect', 'consequ', 'conserv', 'consid', 'consider', 'consist', 'constitut', 'constrain', 'constraint', 'construct', 'consult', 'consum', 'consumpt', 'contact', 'contain', 'contamin', 'contemporari', 'content', 'contin', 'continent', 'contract', 'contrast', 'controversi', 'convect', 'convent', 'convers', 'cool', 'cooper', 'coordin', 'coral', 'corpor', 'correct', 'correl', 'correspond', 'cortic', 'cosmolog', 'could', 'counterpart', 'countri', 'coupl', 'cover', 'creation', 'creativ', 'cross', 'crucial', 'crust', 'crystal', 'cue', 'cultur', 'curricula', 'curriculum', 'curv', 'cut', 'cyber', 'cycl', 'cyprid', 'dark', 'databas', 'dataset', 'date', 'day', 'dc', 'de', 'deal', 'debat', 'decad', 'decay', 'decis', 'decomposit', 'decreas', 'deep', 'deeper', 'defect', 'defens', 'defin', 'deform', 'degre', 'delay', 'deliveri', 'demand', 'democraci', 'democrat', 'demograph', 'demonstr', 'densiti', 'depart', 'depend', 'deploy', 'deposit', 'depth', 'deriv', 'describ', 'descript', 'desir', 'despit', 'detach', 'detail', 'detect', 'detector', 'development', 'devic', 'differenti', 'difficult', 'difficulti', 'diffus', 'digit', 'dimens', 'dimension', 'dioxid', 'disabl', 'disciplin', 'disciplinari', 'discov', 'discoveri', 'discuss', 'diseas', 'display', 'dissemin', 'dissert', 'dissolut', 'diversif', 'dna', 'do', 'doctor', 'document', 'doe', 'domain', 'dome', 'domest', 'domin', 'done', 'doubl', 'down', 'dr', 'dramat', 'draw', 'drawn', 'drill', 'drillcor', 'drive', 'driven', 'drug', 'due', 'duke', 'durat', 'dusel', 'eager', 'ear', 'earliest', 'earmark', 'earthquak', 'easili', 'east', 'ecohydrolog', 'ecolog', 'economi', 'ecosystem', 'edg', 'edit', 'effici', 'egypt', 'eight', 'either', 'elast', 'elect', 'electr', 'electrod', 'electron', 'element', 'elementari', 'elev', 'elsewher', 'elucid', 'embed', 'emerg', 'emiss', 'emot', 'emphasi', 'empir', 'employ', 'encod', 'encount', 'encourag', 'end', 'endeavor', 'engag', 'english', 'enough', 'enrich', 'enrol', 'ensur', 'enterpris', 'entir', 'entiti', 'entitl', 'entrepreneuri', 'equat', 'equatori', 'equilibrium', 'equip', 'equit', 'error', 'especi', 'essenti', 'estim', 'etc', 'ethnic', 'ethnograph', 'european', 'even', 'event', 'evid', 'evolut', 'evolutionari', 'evolv', 'exampl', 'excav', 'excel', 'except', 'excess', 'exchang', 'excit', 'exclus', 'execut', 'exhibit', 'expand', 'expans', 'expens', 'experienc', 'expert', 'expertis', 'explain', 'explicit', 'exploit', 'exploratori', 'expos', 'exposur', 'express', 'extend', 'extens', 'extent', 'extern', 'extinct', 'extract', 'extrem', 'eye', 'fabric', 'face', 'faci', 'facil', 'facilit', 'fact', 'factor', 'faculti', 'failur', 'fall', 'fals', 'famili', 'fan', 'far', 'fast', 'fault', 'favor', 'fe', 'feasibl', 'featur', 'feder', 'feedback', 'fellowship', 'femal', 'fennoscandia', 'few', 'fifteen', 'fill', 'film', 'final', 'financi', 'find', 'finit', 'firm', 'five', 'fix', 'fixat', 'flexibl', 'flight', 'flood', 'flow', 'fluctuat', 'fluid', 'flux', 'fold', 'follow', 'food', 'forc', 'foreign', 'forest', 'formal', 'format', 'former', 'formul', 'forum', 'forward', 'fossil', 'foster', 'found', 'four', 'fragment', 'framework', 'free', 'freeli', 'frequenc', 'friend', 'front', 'frontier', 'fruit', 'fuel', 'full', 'fulli', 'furthermor', 'fusion', 'fy', 'fy09', 'gain', 'game', 'gamma', 'gap', 'gas', 'gather', 'gender', 'gene', 'genet', 'genom', 'geochem', 'geochemistri', 'geograph', 'geolog', 'geometr', 'geometri', 'geophys', 'geoscienc', 'get', 'give', 'given', 'glacial', 'glacier', 'go', 'good', 'govern', 'gps', 'grade', 'gradient', 'grain', 'grant', 'graph', 'graphic', 'great', 'greater', 'grid', 'ground', 'groundwat', 'grow', 'growth', 'guid', 'guidanc', 'had', 'half', 'hand', 'hard', 'hardwar', 'he', 'health', 'heat', 'held', 'henc', 'here', 'heterogen', 'higher', 'highest', 'highlight', 'his', 'histor', 'histori', 'hoc', 'hold', 'hole', 'holocen', 'home', 'hope', 'horizont', 'host', 'hous', 'hundr', 'hybrid', 'hydrodynam', 'hydrogen', 'hydrolog', 'hypothes', 'hypothesi', 'ice', 'idea', 'ideal', 'ident', 'identif', 'ii', 'iii', 'imag', 'immedi', 'immers', 'impli', 'implic', 'implicit', 'inc', 'incent', 'incom', 'incomplet', 'incorpor', 'independ', 'indian', 'indic', 'individu', 'induc', 'infer', 'influenc', 'infrastructur', 'inher', 'inner', 'input', 'insight', 'inspir', 'instabl', 'instal', 'instruct', 'instrument', 'intellig', 'intend', 'intens', 'inter', 'interfac', 'interior', 'internet', 'interpret', 'intertid', 'interv', 'interview', 'introduc', 'invari', 'invent', 'invest', 'invit', 'ion', 'ionospher', 'island', 'isol', 'isotop', 'itali', 'item', 'itself', 'job', 'joint', 'journal', 'junior', 'just', 'keep', 'kind', 'kinet', 'knot', 'know', 'known', 'la', 'lab', 'laboratori', 'lack', 'land', 'landscap', 'languag', 'larger', 'largest', 'larva', 'laser', 'last', 'late', 'later', 'latter', 'lay', 'layer', 'leader', 'leadership', 'least', 'led', 'legal', 'length', 'less', 'letter', 'leverag', 'librari', 'lie', 'life', 'light', 'limit', 'line', 'linear', 'linguist', 'link', 'linkag', 'literatur', 'littl', 'live', 'load', 'locat', 'locomot', 'logic', 'longer', 'longest', 'look', 'loss', 'low', 'lower', 'machin', 'magnet', 'magnitud', 'main', 'maintain', 'mainten', 'maker', 'manifest', 'manifold', 'manipul', 'manner', 'mantl', 'manual', 'manufactur', 'map', 'marin', 'market', 'marketplac', 'mass', 'massiv', 'match', 'math', 'mathemat', 'matrix', 'matter', 'matur', 'maximum', 'maya', 'mean', 'media', 'mediat', 'medic', 'meet', 'melt', 'meltwat', 'member', 'membran', 'memori', 'mentor', 'messag', 'metabol', 'metal', 'meter', 'methodolog', 'metric', 'mexico', 'mhd', 'micro', 'mid', 'middl', 'might', 'migrat', 'militari', 'million', 'mine', 'miner', 'minim', 'minnesota', 'minor', 'miss', 'mission', 'mitig', 'mix', 'mixtur', 'mobil', 'mode', 'modern', 'modifi', 'modul', 'moistur', 'molecul', 'molecular', 'monitor', 'month', 'moreov', 'morpholog', 'motion', 'motiv', 'mountain', 'move', 'movement', 'mri', 'much', 'multidisciplinari', 'multiphas', 'multipl', 'municip', 'museum', 'must', 'mutual', 'name', 'nano', 'nanoscal', 'nasa', 'nativ', 'near', 'necessari', 'negat', 'negoti', 'net', 'neural', 'neuron', 'neurosci', 'neutral', 'neutrino', 'newli', 'next', 'no', 'node', 'noisi', 'nonlinear', 'nonparametr', 'normal', 'north', 'northern', 'notion', 'now', 'nuclear', 'numer', 'nutrient', 'observatori', 'obtain', 'occup', 'occur', 'occurr', 'ocean', 'oceanograph', 'ocr', 'octob', 'off', 'offer', 'offic', 'often', 'oil', 'older', 'onc', 'ongo', 'onlin', 'open', 'optic', 'optim', 'option', 'orbit', 'oregon', 'organiz', 'orient', 'origin', 'os', 'otherwis', 'out', 'outcom', 'output', 'outreach', 'outsid', 'overal', 'overcom', 'overlap', 'own', 'oxid', 'oxygen', 'pace', 'pack', 'pair', 'paleo', 'panel', 'paper', 'paradigm', 'paradox', 'parallel', 'paramet', 'parti', 'partial', 'particl', 'partner', 'partnership', 'pass', 'past', 'patent', 'path', 'pathway', 'patient', 'pattern', 'pay', 'peat', 'peatland', 'peer', 'peopl', 'perceiv', 'percept', 'perhap', 'period', 'perman', 'permit', 'person', 'perspect', 'ph', 'phase', 'phenomena', 'phenomenon', 'phone', 'photon', 'physiolog', 'pi', 'pictur', 'pilot', 'pipelin', 'pis', 'place', 'planet', 'planetari', 'planetarium', 'plant', 'plasma', 'plate', 'plateau', 'platform', 'play', 'player', 'pleistocen', 'plume', 'point', 'polar', 'policymak', 'polit', 'polym', 'polynomi', 'pool', 'poor', 'popul', 'popular', 'portion', 'pose', 'posit', 'positron', 'post', 'postdoc', 'postdoctor', 'power', 'practic', 'practition', 'pre', 'precipit', 'precis', 'prefer', 'preliminari', 'prepar', 'presenc', 'preserv', 'pressur', 'previous', 'price', 'primari', 'primarili', 'primat', 'princip', 'principl', 'prior', 'prioriti', 'privaci', 'privat', 'probe', 'procedur', 'processor', 'profession', 'professor', 'profil', 'profound', 'programm', 'progress', 'promis', 'promot', 'propag', 'proper', 'properti', 'protect', 'protein', 'protocol', 'prototyp', 'prove', 'proven', 'provis', 'proxi', 'proxim', 'psycholog', 'publish', 'puls', 'purpos', 'pursu', 'push', 'qualiti', 'quantifi', 'quantit', 'quantiti', 'quantum', 'queri', 'quit', 'radar', 'radiat', 'radio', 'radioact', 'rais', 'random', 'rapid', 'rare', 'rate', 'rather', 'ration', 'ray', 're', 'reach', 'reaction', 'reactiv', 'real', 'realist', 'reason', 'receiv', 'recogn', 'recognit', 'recommend', 'reconstruct', 'record', 'recov', 'recruit', 'redistribut', 'redox', 'reduct', 'reef', 'refer', 'refin', 'reflect', 'reform', 'regard', 'regim', 'regul', 'regular', 'releas', 'relev', 'reli', 'reliabl', 'remain', 'remot', 'renew', 'repair', 'repeat', 'replac', 'report', 'represent', 'request', 'resid', 'residu', 'resist', 'resolut', 'resolv', 'reson', 'respect', 'respond', 'retent', 'retriev', 'return', 'reveal', 'review', 'rich', 'right', 'rigor', 'rise', 'risk', 'river', 'road', 'robot', 'robust', 'rock', 'root', 'rout', 'routin', 'rule', 'run', 'rural', 'safeti', 'same', 'sampl', 'sand', 'santa', 'satellit', 'sbl', 'scalabl', 'scan', 'scenario', 'schedul', 'scholar', 'sea', 'seafloor', 'search', 'season', 'secondari', 'section', 'sector', 'secur', 'sediment', 'sedimentari', 'see', 'seek', 'seem', 'seen', 'seismic', 'select', 'self', 'semiconductor', 'senior', 'sens', 'sensit', 'sensor', 'separ', 'sequenc', 'sequestr', 'seri', 'serv', 'servic', 'session', 'settlement', 'shallow', 'shape', 'share', 'shear', 'shed', 'shift', 'shore', 'short', 'should', 'show', 'shown', 'siberian', 'signal', 'similar', 'simpl', 'simultan', 'sinc', 'singl', 'site', 'situ', 'situat', 'size', 'skeleton', 'skill', 'slab', 'smart', 'societ', 'socio', 'socioecolog', 'socioeconom', 'softwar', 'soil', 'solar', 'solicit', 'solid', 'solut', 'solv', 'sound', 'south', 'southern', 'span', 'spatial', 'speak', 'speaker', 'speci', 'special', 'specialist', 'specifi', 'specimen', 'spectromet', 'spectroscopi', 'spectrum', 'speech', 'speed', 'spin', 'split', 'spoken', 'sponsor', 'spontan', 'stabil', 'stabl', 'staff', 'stage', 'stakehold', 'standard', 'star', 'start', 'statist', 'status', 'stem', 'step', 'still', 'stimul', 'stochast', 'storag', 'strain', 'strateg', 'strategi', 'stratigraph', 'stratigraphi', 'stream', 'strength', 'strengthen', 'stress', 'stromatolit', 'strong', 'studio', 'sub', 'subject', 'submit', 'subsequ', 'substanti', 'substrat', 'subsurfac', 'suffici', 'suggest', 'suit', 'suitabl', 'sulfur', 'summari', 'summer', 'suppli', 'surf', 'surround', 'survey', 'sustain', 'symposium', 'synergist', 'synthesi', 'synthet', 'systemat', 'take', 'taken', 'talk', 'target', 'task', 'taught', 'taxonomi', 'teach', 'teacher', 'technic', 'tecton', 'telecommun', 'telescop', 'temperatur', 'tempor', 'ten', 'tend', 'terrestri', 'terrorist', 'testb', 'texa', 'theme', 'themselv', 'then', 'theoret', 'therebi', 'therefor', 'thermal', 'thermodynam', 'thick', 'thin', 'think', 'third', 'thousand', 'threat', 'throughout', 'thrust', 'tie', 'today', 'toler', 'top', 'topic', 'topographi', 'topolog', 'total', 'toward', 'trace', 'tracer', 'track', 'tradit', 'traffic', 'trajectori', 'transfer', 'transient', 'transit', 'translat', 'transport', 'trap', 'travel', 'treatment', 'tree', 'tremend', 'triassic', 'trigger', 'tropic', 'tune', 'turbul', 'turn', 'tutori', 'type', 'typic', 'ubiquit', 'uc', 'ultim', 'ultra', 'uncertainti', 'underground', 'underrepres', 'understood', 'undertak', 'unifi', 'uniform', 'unit', 'unknown', 'unobserv', 'unpreced', 'until', 'unusu', 'up', 'upgrad', 'upon', 'upper', 'urban', 'us', 'usabl', 'usc', 'user', 'usual', 'util', 'valid', 'valley', 'valu', 'valuabl', 'vari', 'variabl', 'variat', 'varieti', 'veget', 'vehicl', 'veloc', 'veri', 'verif', 'verifi', 'vertic', 'via', 'video', 'view', 'virginia', 'virtual', 'viscous', 'vision', 'visitor', 'visual', 'vital', 'volcan', 'volum', 'vote', 'vulner', 'wage', 'walk', 'wall', 'war', 'warm', 'was', 'washington', 'water', 'wave', 'weak', 'weather', 'web', 'websit', 'week', 'were', 'west', 'western', 'whether', 'whi', 'whose', 'widespread', 'wimp', 'wind', 'wireless', 'without', 'women', 'word', 'worker', 'workflow', 'workforc', 'worldwid', 'would', 'yet', 'yield', 'you', 'young', 'youth', 'zone']\n"
     ]
    }
   ],
   "source": [
    "chunk = df09[0:500].ABSTRACT\n",
    "       \n",
    "# Create a Snowball stemmer \n",
    "stemmer = SnowballStemmer('english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(chunk):\n",
    "    return (stemmer.stem(w) for w in analyzer(chunk))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                  max_df=0.10,\n",
    "                                  min_df=0.005,\n",
    "                                  max_features = 1500,\n",
    "                                  ngram_range = (0,2),\n",
    "                                  stop_words=modified_stopwords\n",
    "                                 )\n",
    "print(stem_vectorizer.fit_transform(chunk))\n",
    "print(stem_vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(227960, 1500)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1500, 105985), indices imply (1500, 113980)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4859\u001b[0;31m         \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4860\u001b[0m         \u001b[0mmgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, blocks, axes, do_integrity_check)\u001b[0m\n\u001b[1;32m   3281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_integrity_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_verify_integrity\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3492\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_verify_integrity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmgr_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3493\u001b[0;31m                 \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtot_items\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4842\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4843\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1500, 105985), indices imply (1500, 113980)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-df7a63ffd4d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdoc_term_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstem_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# convert to pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mdoc_term_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstem_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;31m# append\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mresult_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_doc_term_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 mgr = self._init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 379\u001b[0;31m                                          copy=copy)\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGeneratorType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_ndarray\u001b[0;34m(self, values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   4864\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4865\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4866\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   4841\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4842\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 4843\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   4844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (1500, 105985), indices imply (1500, 113980)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "chunks = [df09,df10,df11]\n",
    "chunk = chunks[0].ABSTRACT\n",
    "       \n",
    "# Create a Snowball stemmer \n",
    "stemmer = SnowballStemmer('english')\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(chunk):\n",
    "    return (stemmer.stem(w) for w in analyzer(chunk))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words,\n",
    "                                  max_df=0.10,\n",
    "                                  min_df=0.005,\n",
    "                                  max_features = 1500,\n",
    "                                  ngram_range = (0,2),\n",
    "                                  stop_words=modified_stopwords\n",
    "                                 )\n",
    "\n",
    "# # base dataframe\n",
    "# vectorize\n",
    "doc_term_matrix = stem_vectorizer.fit_transform(chunk)\n",
    "\n",
    "# convert to pd\n",
    "base_doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=stem_vectorizer.get_feature_names(),index=chunks[0].PROJECT_ID)\n",
    "\n",
    "# vectorize and append rest of the years\n",
    "for chunk in chunks:\n",
    "    chunk = chunk.ABSTRACT\n",
    "    # vectorize\n",
    "    doc_term_matrix = stem_vectorizer.fit_transform(chunk)\n",
    "    # convert to pd\n",
    "    doc_term_df = pd.DataFrame(doc_term_matrix.toarray(),columns=stem_vectorizer.get_feature_names(),index=chunks[0].PROJECT_ID)\n",
    "    # append\n",
    "    result_raw = base_doc_term_df.append(doc_term_df,sort=False)\n",
    "    # only keep only words/n-grams that appear in every year of data\n",
    "    result_ready = result_raw.dropna(axis=1)\n",
    "    # save result as base in preparation for next loop\n",
    "    base_doc_term_df = result_ready\n",
    "    print(base_doc_term_df.shape)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "# base_doc_term_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
